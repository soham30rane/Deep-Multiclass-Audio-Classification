{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15444,"sourceType":"datasetVersion","datasetId":11102},{"sourceId":9900,"sourceType":"datasetVersion","datasetId":6209},{"sourceId":9157202,"sourceType":"datasetVersion","datasetId":5532072},{"sourceId":192284858,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch.optim as optim\nimport torch.nn.functional as F\n# from torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import models\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n# import time\nimport cv2\nimport os\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:18.954804Z","iopub.execute_input":"2024-08-12T09:59:18.956005Z","iopub.status.idle":"2024-08-12T09:59:18.961503Z","shell.execute_reply.started":"2024-08-12T09:59:18.955956Z","shell.execute_reply":"2024-08-12T09:59:18.960716Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input/cifar10-python'):\n    for foldername in _:\n        print(os.path.join(dirname, foldername))\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:20.956967Z","iopub.execute_input":"2024-08-12T09:59:20.957676Z","iopub.status.idle":"2024-08-12T09:59:20.975619Z","shell.execute_reply.started":"2024-08-12T09:59:20.957646Z","shell.execute_reply":"2024-08-12T09:59:20.974784Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/cifar10-python/cifar-10-batches-py\n","output_type":"stream"}]},{"cell_type":"code","source":"transform = transforms.Compose(\n    [transforms.RandomResizedCrop(255),\n     transforms.CenterCrop(224),  \n     transforms.ToTensor(),\n     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32,\n                                         shuffle=False, num_workers=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:33.319058Z","iopub.execute_input":"2024-08-12T09:59:33.319849Z","iopub.status.idle":"2024-08-12T09:59:34.813168Z","shell.execute_reply.started":"2024-08-12T09:59:33.319816Z","shell.execute_reply":"2024-08-12T09:59:34.812460Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"train_len=len(trainset)\ntest_len=len(testset)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:36.838503Z","iopub.execute_input":"2024-08-12T09:59:36.838916Z","iopub.status.idle":"2024-08-12T09:59:36.846574Z","shell.execute_reply.started":"2024-08-12T09:59:36.838884Z","shell.execute_reply":"2024-08-12T09:59:36.845036Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(test_len)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:39.030972Z","iopub.execute_input":"2024-08-12T09:59:39.031653Z","iopub.status.idle":"2024-08-12T09:59:39.036117Z","shell.execute_reply.started":"2024-08-12T09:59:39.031621Z","shell.execute_reply":"2024-08-12T09:59:39.035202Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"10000\n","output_type":"stream"}]},{"cell_type":"code","source":"model = models.resnet50(pretrained=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:42.571159Z","iopub.execute_input":"2024-08-12T09:59:42.571948Z","iopub.status.idle":"2024-08-12T09:59:43.862526Z","shell.execute_reply.started":"2024-08-12T09:59:42.571918Z","shell.execute_reply":"2024-08-12T09:59:43.861600Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 160MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"for param in model.parameters(): # freezes the layers\n    param.required_grad = False\nnumber_feature = model.fc.in_features\nmodel.fc = torch.nn.Linear(in_features=number_feature , out_features=10, bias=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:48.859753Z","iopub.execute_input":"2024-08-12T09:59:48.860159Z","iopub.status.idle":"2024-08-12T09:59:48.867306Z","shell.execute_reply.started":"2024-08-12T09:59:48.860127Z","shell.execute_reply":"2024-08-12T09:59:48.866135Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device is ', device)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:53.672487Z","iopub.execute_input":"2024-08-12T09:59:53.672853Z","iopub.status.idle":"2024-08-12T09:59:53.734849Z","shell.execute_reply.started":"2024-08-12T09:59:53.672824Z","shell.execute_reply":"2024-08-12T09:59:53.733969Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"device is  cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:59:57.014016Z","iopub.execute_input":"2024-08-12T09:59:57.014918Z","iopub.status.idle":"2024-08-12T09:59:57.257545Z","shell.execute_reply.started":"2024-08-12T09:59:57.014883Z","shell.execute_reply":"2024-08-12T09:59:57.256651Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=10, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer=optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T10:00:02.410158Z","iopub.execute_input":"2024-08-12T10:00:02.410745Z","iopub.status.idle":"2024-08-12T10:00:02.416678Z","shell.execute_reply.started":"2024-08-12T10:00:02.410711Z","shell.execute_reply":"2024-08-12T10:00:02.415575Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model.train()\n    \nfor epoch in range(15):\n    total_correct = 0.0 \n    running_loss = 0.0 \n    for i, (inputs, labels) in enumerate(trainloader):\n        inputs, labels = inputs.cuda(), labels.cuda()\n        # Forward pass\n        output = model(inputs)\n        # Get predicted classes\n        output_idex = torch.argmax(output, dim=1)\n        # Calculate the number of correct predictions\n        total_correct += (labels == output_idex).sum().item()\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        # Calculate loss\n        loss = criterion(output, labels)  # Fixed typo: 'cirterion' to 'criterion'\n        running_loss += loss.item() * inputs.size(0)\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n    print('epoch: ', epoch, 'loss: ', running_loss / train_len, 'accuracy: ', (total_correct / train_len) * 100, '%')\n\n    if (epoch + 1) % 3 == 0:\n            Data_Snapshot = f'/kaggle/working/cnnnet_epoch_{epoch+1}.pth'\n            torch.save(model.state_dict(), Data_Snapshot)\n            print(f\" saved at {Data_Snapshot}\")\n    print(\"---------------------------\")\n\nprint('finished training')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T10:02:21.388518Z","iopub.execute_input":"2024-08-12T10:02:21.389054Z","iopub.status.idle":"2024-08-12T10:02:21.408761Z","shell.execute_reply.started":"2024-08-12T10:02:21.388969Z","shell.execute_reply":"2024-08-12T10:02:21.407585Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"498.9s 14 epoch:  0 loss:  1.6747970003509522 accuracy:  38.506 %\n498.9s 15 ---------------------------\n978.6s 16 epoch:  1 loss:  1.334530128326416 accuracy:  52.318 %\n978.6s 17 ---------------------------\n1457.4s 18 epoch:  2 loss:  1.1832898192214967 accuracy:  58.104 %\n1457.4s 19 saved at /kaggle/working/cnnnet_epoch_3.pth\n1457.4s 20 ---------------------------\n1935.5s 21 epoch:  3 loss:  1.0852978984069823 accuracy:  61.692 %\n1935.5s 22 ---------------------------\n2412.5s 23 epoch:  4 loss:  1.0109545670890807 accuracy:  64.364 %\n2412.5s 24 ---------------------------\n2888.3s 25 epoch:  5 loss:  0.94886798620224 accuracy:  66.806 %\n2888.3s 26 saved at /kaggle/working/cnnnet_epoch_6.pth\n2888.3s 27 ---------------------------\n3363.0s 28 epoch:  6 loss:  0.8991863755226135 accuracy:  68.794 %\n3363.0s 29 ---------------------------\n3836.9s 30 epoch:  7 loss:  0.8649814581489563 accuracy:  69.746 %\n3836.9s 31 ---------------------------\n4309.9s 32 epoch:  8 loss:  0.8314949030303955 accuracy:  71.154 %\n4309.9s 33 saved at /kaggle/working/cnnnet_epoch_9.pth\n4309.9s 34 ---------------------------\n4780.5s 35 epoch:  9 loss:  0.8040117253112793 accuracy:  71.934 %\n4780.5s 36 ---------------------------\n5249.8s 37 epoch:  10 loss:  0.7802304533195495 accuracy:  72.916 %\n5249.8s 38 ---------------------------\n5718.2s 39 epoch:  11 loss:  0.761436590309143 accuracy:  73.50999999999999 %\n5718.2s 40 saved at /kaggle/working/cnnnet_epoch_12.pth\n5718.2s 41 ---------------------------\n6185.4s 42 epoch:  12 loss:  0.7427867577934265 accuracy:  74.114 %\n6185.4s 43 ---------------------------\n6651.7s 44 epoch:  13 loss:  0.7226831003379822 accuracy:  74.87 %\n6651.7s 45 ---------------------------\n7117.5s 46 epoch:  14 loss:  0.7120205615234375 accuracy:  75.33999999999999 %\n7117.5s 47 saved at /kaggle/working/cnnnet_epoch_15.pth\n7117.5s 48 ---------------------------\n7117.5s 49 finished training\n7120.9s 50 /opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2930: FutureWarning: --Exporter.preprocessors=[\"remove_papermill_header.RemovePapermillHeader\"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.\n7120.9s 51 warn(\n7121.0s 52 [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n7121.0s 53 [NbConvertApp] Converting notebook __notebook__.ipynb to notebook\n7121.4s 54 [NbConvertApp] Writing 30037 bytes to __notebook__.ipynb\n7123.0s 55 /opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2930: FutureWarning: --Exporter.preprocessors=[\"nbconvert.preprocessors.ExtractOutputPreprocessor\"] for containers is deprecated in traitlets 5.0. You can pass `--Exporter.preprocessors item` ... multiple times to add items to a list.\n7123.0s 56 warn(\n7123.0s 57 [NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n7123.1s 58 [NbConvertApp] Converting notebook __notebook__.ipynb to html\n7124.0s 59 [NbConvertApp] Writing 307722 bytes to __results__.html\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test\n\nwith torch.no_grad():\n    model.eval()\n    total_loss=0.0 \n    total_correct=0.0 \n    for inputs, labels in testloader:\n        labels=labels.to(device)\n        outputs=model(inputs.to(device))\n        loss=criterion(outputs, labels)\n        total_loss+=loss.item()*inputs.size(0)\n        output_idx=torch.argmax(outputs, dim=1)\n        total_correct+=sum(labels==output_idx).sum().item() \n        accuracy = (total_correct / test_len) * 100\n        loss = total_loss / test_len\n\n    print(f'Accuracy: {accuracy:.2f}% Loss: {loss:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-08-12T10:18:02.649428Z","iopub.execute_input":"2024-08-12T10:18:02.650132Z","iopub.status.idle":"2024-08-12T10:18:02.655144Z","shell.execute_reply.started":"2024-08-12T10:18:02.650100Z","shell.execute_reply":"2024-08-12T10:18:02.654323Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Accuracy: 50.02% Loss: 2.3496\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}